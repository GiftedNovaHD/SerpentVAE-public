# Training configuration
dataset_path: "salesforce/wikitext"
dataset_name: "wikitext-103-raw-v1"
num_epochs: 10
eval_freq: 0.001
checkpoint_freq: 50
model_save_folder_path: "discrete-debug-model-ckpt"
metrics_save_folder_path: "discrete-debug-metrics-ckpt"
training_path: "discrete-debug-train-ckpt"
learning_rate: 1e-4
min_learning_rate: 1e-8
weight_decay: 1e-2
max_seq_len: 64
batch_size: 2
dataloader_num_workers: 16
parallelism_strategy: "ddp"
compile_model: true
is_debug: true
seed: 6969

# Model configuration
# Notes on model configuration:
# For Mamba2 head_dims, hidden_dim/concept_dim * mamba_expand / head_dim must be a multiple of 8

# Global variables
hidden_dim: 512
concept_dim: 512
vocab_size: 129280 # NOTE: Vocab size can be left blank to take in continuous inputs
input_dim: # NOTE: Input dimension can be left blank for continuous inputs

# Reconstruction loss configuration
recon_loss_name: "CE"
recon_loss_reduction: "mean"

# Training scale factors
alpha: 1.0 # This scales the VMI regularization term
beta: 1.0 # This scales the KL divergence term
ema_decay_factor: 0.75 # This is the decay factor for the EMA of the averaqe subsequence length

# Embedding configuration
# NOTE: This is for both continuous and discrete inputs
share_input_embeddings: true
tie_embeddings: true

# Hardware configuration
residual_in_fp32: false
device: "cuda"
dtype: "bf16"

# Model configuration
# Boundary operator configuration
boundary_operator:
  ChainCRP:
    use_odds_ratio: false
    compression_strength: 1.0


# Replacement function configuration
replacement_function: "use_last"

# Distribution configuration
distribution:
  ScaledNormal:
    dist_desired_std: 1.0

# Encoder configuration
encoder:
  # Non-sequence mixer settings
  seq_mixer_layer_config: "2 (M2)" 
  channel_mixer_layer_config: "2 (MLP)"
  segment_pred_inner_dim: 768 # Encoder segment predictor inner dimension
  num_segment_predictions: 1 # Number of segment predictions to make

  # Sequence mixer configuration
  Mamba2:
    alias: "M2"
    mamba2_state_dim: 8
    mamba2_conv_length: 4
    mamba2_expand: 2
    mamba2_head_dim: 64 # (hidden_dim * mamba_expand / mamba_head_dim = 64 = 8 * 8)

  # Channel mixer configuration
  MLP:
    mlp_inner_dim: 768

# Decoder configuration
decoder:
  # Non-sequence mixer settings
  seq_mixer_layer_config: "2 (Mamba2)"
  channel_mixer_layer_config: "2 (MLP)"
  segment_pred_inner_dim: 768 # Decoder segment predictor inner dimension
  num_segment_predictions: 1 # Number of segment predictions to make
  
  # Sequence mixer configuration
  Mamba2:
    mamba2_state_dim: 8
    mamba2_conv_length: 4
    mamba2_expand: 2
    mamba2_head_dim: 64 # (hidden_dim * mamba_expand / mamba_head_dim = 64 = 8 * 8)

  # Channel mixer configuration
  MLP:
    mlp_inner_dim: 768

# Confidence Module configuration
enable_confidence_module: true # This module is used for Stochastic Varitional Inference (not implemented yet)
confidence_module:
  mlp_inner_dim: 768

# Q-Net configuration
enable_qnet: true # We advise Q-Net to be enabled to reduce the chance of posterior collapse
qnet:
  # Non-sequence mixer settings
  seq_mixer_layer_config: "Mamba2"
  channel_mixer_layer_config: "MLP"
  
  # Sequence mixer configuration
  Mamba2:
    mamba2_conv_length: 4
    mamba2_expand: 1
    mamba2_head_dim: 32 # (concept_dim * mamba_expand / qnet_mamba_head_dim = 48 = 8 * 6)
    mamba2_state_dim: 8

  # Channel mixer configuration
  MLP:
    mlp_inner_dim: 384
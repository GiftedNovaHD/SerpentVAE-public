# Training configuration
dataset_path: "GiftedNova/UCF101-trial"
dataset_name: 
num_epochs: 50
eval_freq: 0.0002
checkpoint_freq: 5
model_save_folder_path: "video-debug-model-ckpt"
metrics_save_folder_path: "video-debug-metrics-ckpt"
training_path: "video-debug-train-ckpt"
learning_rate: 1e-4
min_learning_rate: 1e-8
weight_decay: 1e-2
max_seq_len: 128
batch_size: 2
dataloader_num_workers: 1
parallelism_strategy: "ddp"
compile_model: true
is_debug: true
seed: 6969

# Model configuration
# Notes on model configuration:
# For Mamba2 head_dims, hidden_dim/concept_dim * mamba_expand / head_dim must be a multiple of 8

# Global variables
hidden_dim: 512
concept_dim: 512
vocab_size: # NOTE: Vocab size can be left blank to take in continuous inputs
input_dim: 16 # NOTE: Input dimension can be left blank for continuous inputs

# Special token ids only needed for discrete inputs
special_token_ids:

# Reconstruction loss configuration
recon_loss_name: "RMSE"
recon_loss_reduction: "mean"

# Training scale factors
alpha: 1.0 # This scales the VMI regularization term
beta: 0.5 # This scales the KL divergence term
ema_decay_factor: 0.75 # This is the decay factor for the EMA of the averaqe subsequence length

# Embedding configuration
# NOTE: This is for both continuous and discrete inputs
share_input_embeddings: true
tie_embeddings: true

# Hardware configuration
residual_in_fp32: false
device: "cuda"
dtype: "bf16"

# Model configuration
# Boundary operator configuration
boundary_operator:
  ChainCRP:
    use_odds_ratio: false
    compression_strength: 0.2
    recon_threshold: 0.4 # Only below this threshold will we try to increase the subsequence length

# Replacement function configuration
replacement_function: "use_last"

# Distribution configuration
distribution:
  ScaledNormal:
    dist_desired_std: 1.0

# Encoder configuration
encoder:
  # Non-sequence mixer settings
  seq_mixer_layer_config: "4 (M2)"
  channel_mixer_layer_config: "4 (MLP)"
  segment_pred_inner_dim: 1536 # Encoder segment predictor inner dimension
  num_segment_predictions: 2 # Number of segment predictions to make

  # Sequence mixer configuration
  Mamba2:
    alias: "M2"
    mamba2_state_dim: 16
    mamba2_conv_length: 4
    mamba2_expand: 2
    mamba2_head_dim: 16 # (hidden_dim * mamba_expand / mamba_head_dim = 64 = 8 * 8)

  # Channel mixer configuration
  MLP:
    mlp_inner_dim: 1536  # Inner dimension for MLP channel mixer

# Decoder configuration
decoder:
  # Non-sequence mixer settings
  seq_mixer_layer_config: "4 (Mamba2)"
  channel_mixer_layer_config: "4 (MLP)"
  segment_pred_inner_dim: 1536 # Decoder segment predictor inner dimension
  num_segment_predictions: 2 # Number of segment predictions to make
  
  # Sequence mixer configuration
  Mamba2:
    mamba2_state_dim: 16
    mamba2_conv_length: 4
    mamba2_expand: 2
    mamba2_head_dim: 16 # (hidden_dim * mamba_expand / mamba_head_dim = 64 = 8 * 8)

  # Channel mixer configuration
  MLP:
    mlp_inner_dim: 1536

# Confidence Module configuration
enable_confidence_module: false # This module is used for Stochastic Varitional Inference (not implemented yet)
confidence_module:
  mlp_inner_dim: 1536

# Q-Net configuration
enable_qnet: false # We advise Q-Net to be enabled to reduce the chance of posterior collapse
qnet:
  # Non-sequence mixer settings
  seq_mixer_layer_config: "2 (Mamba2)"
  channel_mixer_layer_config: "2 (MLP)"
  
  # Sequence mixer configuration
  Mamba2:
    mamba2_conv_length: 4
    mamba2_expand: 1
    mamba2_head_dim: 32 # (concept_dim * mamba_expand / qnet_mamba_head_dim = 48 = 8 * 6)
    mamba2_state_dim: 16

  # Channel mixer configuration
  MLP:
    mlp_inner_dim: 384
# Training configuration
dataset_path: "ETDataset/ett"
dataset_name: "m1"
num_epochs: 1000
eval_freq: 1
checkpoint_freq: 50
model_save_folder_path: "time-series-debug-model-ckpt"
metrics_save_folder_path: "time-series-debug-metrics-ckpt"
training_path: "time-series-debug-train-ckpt"
learning_rate: 1e-4
min_learning_rate: 1e-7
weight_decay: 1e-2
max_seq_len: 64
batch_size: 16
dataloader_num_workers: 16
parallelism_strategy: "ddp"
compile_model: true
is_debug: true
seed: 6969

# Model configuration
# Notes on model configuration:
# For Mamba2 head_dims, hidden_dim/concept_dim * mamba_expand / head_dim must be a multiple of 8

# Global variables
hidden_dim: 32
concept_dim: 16
vocab_size: # NOTE: Vocab size can be left blank to take in continuous inputs
input_dim: 7 # NOTE: Input dimension can be left blank for continuous inputs

# Special token ids only needed for discrete inputs
special_token_ids:

# Reconstruction loss configuration
recon_loss_name: "RMSE"
recon_loss_reduction: "mean"

# Training scale factors
alpha: 1.0 # This scales the VMI regularization term
beta: 0.5 # This scales the KL divergence term
ema_decay_factor: 0.75 # This is the decay factor for the EMA of the averaqe subsequence length

# Embedding configuration
# NOTE: This is for both continuous and discrete inputs
share_input_embeddings: true
tie_embeddings: true

# Hardware configuration
residual_in_fp32: false
device: "cuda"
dtype: "bf16"

# Model configuration
# Boundary operator configuration
boundary_operator:
  ChainCRP:
    use_odds_ratio: false
    compression_strength: 0.02
    recon_threshold: 0.4 # Only below this threshold will we try to increase the subsequence length


# Replacement function configuration
replacement_function: "use_last"

# Distribution configuration
distribution:
  ScaledNormal:
    dist_desired_std: 0.75

# Encoder configuration
encoder:
  # Non-sequence mixer settings
  seq_mixer_layer_config: "3 (M2)" 
  channel_mixer_layer_config: "3 (MLP)"
  segment_pred_inner_dim: 256 # Encoder segment predictor inner dimension
  num_segment_predictions: 2 # Number of segment predictions to make

  # Sequence mixer configuration
  Mamba2:
    alias: "M2"
    mamba2_conv_length: 4
    mamba2_expand: 4
    mamba2_head_dim: 4 # (hidden_dim * mamba_expand / qnet_mamba_head_dim = 8)
    mamba2_state_dim: 16

  # Channel mixer configuration
  MLP:
    mlp_inner_dim: 32

# Decoder configuration
decoder:
  # Non-sequence mixer settings
  seq_mixer_layer_config: "2 (M2)"
  channel_mixer_layer_config: "2 (MLP)"
  segment_pred_inner_dim: 384 # Decoder segment predictor inner dimension
  num_segment_predictions: 2 # Number of segment predictions to make
  
  # Sequence mixer configuration
  Mamba2:
    alias: "M2"
    mamba2_conv_length: 4
    mamba2_expand: 4
    mamba2_head_dim: 4 # (hidden_dim * mamba_expand / qnet_mamba_head_dim = 8)
    mamba2_state_dim: 16

  # Channel mixer configuration
  MLP:
    mlp_inner_dim: 32

# Confidence Module configuration
enable_confidence_module: false # This module is used for Stochastic Varitional Inference (not implemented yet)
confidence_module:
  mlp_inner_dim: 96

# Q-Net configuration
enable_qnet: false # We advise Q-Net to be enabled to reduce the chance of posterior collapse
qnet:
  # Non-sequence mixer settings
  seq_mixer_layer_config: "Mamba2"
  channel_mixer_layer_config: "MLP"
  
  # Sequence mixer configuration
  Mamba2:
    mamba2_conv_length: 4
    mamba2_expand: 2
    mamba2_head_dim: 4 # (concept_dim * mamba_expand / qnet_mamba_head_dim = 8)
    mamba2_state_dim: 16

  # Channel mixer configuration
  MLP:
    mlp_inner_dim: 48

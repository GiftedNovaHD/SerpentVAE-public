# Training configuration
dataset_path: "salesforce/wikitext"
dataset_name: "wikitext-2-raw-v1"
num_epochs: 10
eval_freq: 1
model_save_folder_path:
metrics_save_folder_path: 
training_path: 
learning_rate: 1e-4
min_learning_rate: 1e-8
weight_decay: 1e-2
max_seq_len: 64
batch_size: 2
parallelism_strategy: "fsdp"
compile_model: true
seed: 6969

# Model configuration
# Notes on model configuration:
# For Mamba2 head_dims, hidden_dim/concept_dim * mamba_expand / head_dim must be a multiple of 8

# Model configuration settings
hidden_dim: 512
concept_dim: 512
vocab_size: 129280
dist_desired_std: 1.0
num_encoder_layers: 2
num_decoder_layers: 2
mamba_state_dim: 8
mamba_conv_length: 4
mamba_expand: 2
mamba_head_dim: 64 # (hidden_dim * mamba_expand / mamba_head_dim = 64 = 8 * 8)
mlp_inner_dim: 768
confidence_inner_dim: 768
segment_pred_inner_dim: 768

# Q-Net configuration
enable_qnet: true # We advise Q-Net to be enabled to reduce the chance of posterior collapse
num_qnet_layers: 1
qnet_conv_length: 4
qnet_mamba_expand: 1
qnet_mamba_head_dim: 48 # (concept_dim * mamba_expand / qnet_mamba_head_dim = 48 = 8 * 6)
qnet_mlp_inner_dim: 384
qnet_mamba_state_dim: 8

# Embedding configuration
share_input_embeddings: true
tie_embeddings: true

# Hardware configuration
residual_in_fp32: false
device: "cuda"
dtype: "bf16"
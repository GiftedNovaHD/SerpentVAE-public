# Training configuration
dataset_path: "salesforce/wikitext"
dataset_name: "wikitext-2-raw-v1"
num_epochs: 10
eval_freq: 1
model_save_folder_path:
metrics_save_folder_path: 
training_path: 
learning_rate: 1e-4
min_learning_rate: 1e-8
weight_decay: 1e-2
max_seq_len: 64
batch_size: 2
parallelism_strategy: "fsdp"
compile_model: true
seed: 6969

# Model configuration
# Notes on model configuration:
# For Mamba2 head_dims, hidden_dim/concept_dim * mamba_expand / head_dim must be a multiple of 8

# Global variables
hidden_dim: 512
concept_dim: 512
vocab_size: 129280 # NOTE: Vocab size can be left blank to take in continuous inputs
confidence_inner_dim: 768

# Embedding configuration
share_input_embeddings: true
tie_embeddings: true

# Hardware configuration
residual_in_fp32: false
device: "cuda"
dtype: "bf16"

# Model configuration settings
distribution:
  ScaleVAE:
    dist_desired_std: 1.0

encoder:
  # Non-sequence mixer settings
  layer_config: "2 (M2)" 
  mlp_inner_dim: 768
  segment_pred_inner_dim: 768 # Encoder segment predictor inner dimension

  # Sequence mixer configuration
  Mamba2:
    alias: "M2"
    mamba2_state_dim: 8
    mamba2_conv_length: 4
    mamba2_expand: 2
    mamba2_head_dim: 64 # (hidden_dim * mamba_expand / mamba_head_dim = 64 = 8 * 8)

decoder:
  # Non-sequence mixer settings
  layer_config: "2 (Mamba2)"
  mlp_inner_dim: 768
  confidence_inner_dim: 768
  segment_pred_inner_dim: 768 # Decoder segment predictor inner dimension

  # Sequence mixer configuration
  Mamba2:
    mamba2_state_dim: 8
    mamba2_conv_length: 4
    mamba2_expand: 2
    mamba2_head_dim: 64 # (hidden_dim * mamba_expand / mamba_head_dim = 64 = 8 * 8)

  

# Q-Net configuration
enable_qnet: true # We advise Q-Net to be enabled to reduce the chance of posterior collapse
qnet:
  # Non-sequence mixer settings
  layer_config: "Mamba2"
  qnet_mlp_inner_dim: 384
  
  # Sequence mixer configuration
  Mamba2:
    mamba2_conv_length: 4
    mamba2_expand: 1
    mamba2_head_dim: 48 # (concept_dim * mamba_expand / qnet_mamba_head_dim = 48 = 8 * 6)
    mamba2_state_dim: 8
  
  


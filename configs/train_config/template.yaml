# Training configuration
dataset_path:
dataset_name:
num_epochs:
eval_freq:
model_save_folder_path:
metrics_save_folder_path: 
training_path: 
learning_rate:
min_learning_rate:
weight_decay:
max_seq_len:
batch_size:
dataloader_num_workers:
parallelism_strategy: "fsdp"
compile_model: true
seed:

# Model configuration
# Notes on model configuration:
# For Mamba2 head_dims, hidden_dim/concept_dim * mamba_expand / head_dim must be a multiple of 8

# Global variables
hidden_dim:
concept_dim:
vocab_size: # NOTE: Vocab size can be left blank to take in continuous inputs

# Training scale factors
alpha: 1.0 # This scales the VMI regularization term
beta: 1.0 # This scales the KL divergence term
ema_decay_factor: 0.75 # This is the decay factor for the EMA of the averaqe subsequence length

# Embedding configuration
share_input_embeddings: true
tie_embeddings: true

# Hardware configuration
residual_in_fp32: false
device: "cuda"
dtype: "bf16"

# Model configuration
# ChainCRP configuration
use_odds_ratio: false

# Distribution configuration
distribution:
  

# Encoder configuration
encoder:
  # Non-sequence mixer settings
  layer_config:
  mlp_inner_dim:
  segment_pred_inner_dim: # Encoder segment predictor inner dimension

  # Sequence mixer configuration

# Decoder configuration
decoder:
  # Non-sequence mixer settings
  layer_config:
  mlp_inner_dim:
  segment_pred_inner_dim: # Decoder segment predictor inner dimension

  # Sequence mixer configuration

# Confidence Module configuration
enable_confidence_module: true # This module is used for Stochastic Varitional Inference (not implemented yet)
confidence_module:
  mlp_inner_dim:

# Q-Net configuration
enable_qnet: true # We advise Q-Net to be enabled to reduce the chance of posterior collapse
qnet:
  # Non-sequence mixer settings
  layer_config:
  mlp_inner_dim:
  
  # Sequence mixer configuration
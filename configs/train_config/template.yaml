# Training configuration
dataset_path:
dataset_name:
num_epochs:
eval_freq:
model_save_folder_path:
metrics_save_folder_path:
training_path:
learning_rate:
min_learning_rate:
weight_decay:
max_seq_len:
batch_size:
parallelism_strategy: "fsdp"
compile_model: true
seed:

# Model configuration
# Notes on model configuration:
# For Mamba2 head_dims, hidden_dim/concept_dim * mamba_expand / head_dim must be a multiple of 8

# Global variables
hidden_dim:
concept_dim:
vocab_size: # NOTE: Vocab size can be left blank to take in continuous inputs

# Embedding configuration
share_input_embeddings: true
tie_embeddings: true

# Hardware configuration
residual_in_fp32: false
device: "cuda"
dtype:

# Model configuration settings
distribution:
  

encoder:
  # Non-sequence mixer settings
  layer_config: 
  mlp_inner_dim:
  segment_pred_inner_dim: # Encoder segment predictor inner dimension

  # Sequence mixer configuration

decoder:
  # Non-sequence mixer settings
  layer_config:
  mlp_inner_dim:
  segment_pred_inner_dim: # Decoder segment predictor inner dimension

  # Sequence mixer configuration
  

# Confidence Module configuration
confidence_module:
  mlp_inner_dim:

# Q-Net configuration
enable_qnet: true # We advise Q-Net to be enabled to reduce the chance of posterior collapse
qnet:
  # Non-sequence mixer settings
  layer_config:
  mlp_inner_dim:
  
  # Sequence mixer configuration
  